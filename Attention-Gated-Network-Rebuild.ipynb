{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is to rebuild the attention gated network for segmentation from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1.[Libraries](#library)\n",
    "\n",
    "2.[Dataloader](#dataloader)\n",
    "\n",
    "3.[Data Augmentation](#augmentation)\n",
    "\n",
    "4.[Network Building](#network)\n",
    " - [Architecture](#architecture)\n",
    " - [Layer](#layer)\n",
    " - [Grid Attention Layer](#attentionLayer)\n",
    " \n",
    "5.[Training](#training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='library'></a>\n",
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T12:32:12.229374Z",
     "start_time": "2018-12-16T12:32:12.216054Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='network'></a>\n",
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T12:33:56.111805Z",
     "start_time": "2018-12-16T12:33:56.081447Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3D Convolutional Unet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetConv3(nn.Module):\n",
    "    def __init__(self, in_size, out_size, is_batchnorm, kernel_size=(3,3,1), padding_size=(1,1,0), init_stride=(1,1,1)):\n",
    "        super(UnetConv3, self).__init__()\n",
    "\n",
    "        if is_batchnorm:\n",
    "            self.conv1 = nn.Sequential(nn.Conv3d(in_size, out_size, kernel_size, init_stride, padding_size),\n",
    "                                       nn.BatchNorm3d(out_size),\n",
    "                                       nn.ReLU(inplace=True),)\n",
    "            self.conv2 = nn.Sequential(nn.Conv3d(out_size, out_size, kernel_size, 1, padding_size),\n",
    "                                       nn.BatchNorm3d(out_size),\n",
    "                                       nn.ReLU(inplace=True),)\n",
    "        else:\n",
    "            self.conv1 = nn.Sequential(nn.Conv3d(in_size, out_size, kernel_size, init_stride, padding_size),\n",
    "                                       nn.ReLU(inplace=True),)\n",
    "            self.conv2 = nn.Sequential(nn.Conv3d(out_size, out_size, kernel_size, 1, padding_size),\n",
    "                                       nn.ReLU(inplace=True),)\n",
    "\n",
    "        # initialise the blocks\n",
    "        for m in self.children():\n",
    "            init_weights(m, init_type='kaiming')\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.conv1(inputs)\n",
    "        outputs = self.conv2(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3D Fully Convolutional Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T12:35:40.423709Z",
     "start_time": "2018-12-16T12:35:40.381096Z"
    }
   },
   "outputs": [],
   "source": [
    "class FCNConv3(nn.Module):\n",
    "    def __init__(self, in_size, out_size, is_batchnorm, kernel_size=(3,3,1), padding_size=(1,1,0), init_stride=(1,1,1)):\n",
    "        super(FCNConv3, self).__init__()\n",
    "\n",
    "        if is_batchnorm:\n",
    "            self.conv1 = nn.Sequential(nn.Conv3d(in_size, out_size, kernel_size, init_stride, padding_size),\n",
    "                                       nn.BatchNorm3d(out_size),\n",
    "                                       nn.ReLU(inplace=True),)\n",
    "            self.conv2 = nn.Sequential(nn.Conv3d(out_size, out_size, kernel_size, 1, padding_size),\n",
    "                                       nn.BatchNorm3d(out_size),\n",
    "                                       nn.ReLU(inplace=True),)\n",
    "            self.conv3 = nn.Sequential(nn.Conv3d(out_size, out_size, kernel_size, 1, padding_size),\n",
    "                                       nn.BatchNorm3d(out_size),\n",
    "                                       nn.ReLU(inplace=True),)\n",
    "        else:\n",
    "            self.conv1 = nn.Sequential(nn.Conv3d(in_size, out_size, kernel_size, init_stride, padding_size),\n",
    "                                       nn.ReLU(inplace=True),)\n",
    "            self.conv2 = nn.Sequential(nn.Conv3d(out_size, out_size, kernel_size, 1, padding_size),\n",
    "                                       nn.ReLU(inplace=True),)\n",
    "            self.conv3 = nn.Sequential(nn.Conv3d(out_size, out_size, kernel_size, 1, padding_size),\n",
    "                                       nn.ReLU(inplace=True),)\n",
    "\n",
    "        # initialise the blocks\n",
    "        for m in self.children():\n",
    "            init_weights(m, init_type='kaiming')\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.conv1(inputs)\n",
    "        outputs = self.conv2(outputs)\n",
    "        outputs = self.conv3(outputs)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unet Gating Signal Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T12:38:43.405666Z",
     "start_time": "2018-12-16T12:38:43.366006Z"
    }
   },
   "outputs": [],
   "source": [
    "class UnetGatingSignal3(nn.Module):\n",
    "    def __init__(self, in_size, out_size, is_batchnorm):\n",
    "        super(UnetGatingSignal3, self).__init__()\n",
    "        self.fmap_size = (4, 4, 4)\n",
    "\n",
    "        if is_batchnorm:\n",
    "            self.conv1 = nn.Sequential(nn.Conv3d(in_size, in_size//2, (1,1,1), (1,1,1), (0,0,0)),\n",
    "                                       nn.BatchNorm3d(in_size//2),\n",
    "                                       nn.ReLU(inplace=True),\n",
    "                                       nn.AdaptiveAvgPool3d(output_size=self.fmap_size),\n",
    "                                       )\n",
    "            self.fc1 = nn.Linear(in_features=(in_size//2) * self.fmap_size[0] * self.fmap_size[1] * self.fmap_size[2],\n",
    "                                 out_features=out_size, bias=True)\n",
    "        else:\n",
    "            self.conv1 = nn.Sequential(nn.Conv3d(in_size, in_size//2, (1,1,1), (1,1,1), (0,0,0)),\n",
    "                                       nn.ReLU(inplace=True),\n",
    "                                       nn.AdaptiveAvgPool3d(output_size=self.fmap_size),\n",
    "                                       )\n",
    "            self.fc1 = nn.Linear(in_features=(in_size//2) * self.fmap_size[0] * self.fmap_size[1] * self.fmap_size[2],\n",
    "                                 out_features=out_size, bias=True)\n",
    "\n",
    "        # initialise the blocks\n",
    "        for m in self.children():\n",
    "            init_weights(m, init_type='kaiming')\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.size(0)\n",
    "        outputs = self.conv1(inputs)\n",
    "        outputs = outputs.view(batch_size, -1)\n",
    "        outputs = self.fc1(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unet Grid Gating Signal Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T12:39:24.880189Z",
     "start_time": "2018-12-16T12:39:24.861243Z"
    }
   },
   "outputs": [],
   "source": [
    "class UnetGridGatingSignal3(nn.Module):\n",
    "    def __init__(self, in_size, out_size, kernel_size=(1,1,1), is_batchnorm=True):\n",
    "        super(UnetGridGatingSignal3, self).__init__()\n",
    "\n",
    "        if is_batchnorm:\n",
    "            self.conv1 = nn.Sequential(nn.Conv3d(in_size, out_size, kernel_size, (1,1,1), (0,0,0)),\n",
    "                                       nn.BatchNorm3d(out_size),\n",
    "                                       nn.ReLU(inplace=True),\n",
    "                                       )\n",
    "        else:\n",
    "            self.conv1 = nn.Sequential(nn.Conv3d(in_size, out_size, kernel_size, (1,1,1), (0,0,0)),\n",
    "                                       nn.ReLU(inplace=True),\n",
    "                                       )\n",
    "\n",
    "        # initialise the blocks\n",
    "        for m in self.children():\n",
    "            init_weights(m, init_type='kaiming')\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.conv1(inputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3D CT Unet Up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T12:45:44.354242Z",
     "start_time": "2018-12-16T12:45:44.335256Z"
    }
   },
   "outputs": [],
   "source": [
    "class UnetUp3_CT(nn.Module):\n",
    "    def __init__(self, in_size, out_size, is_batchnorm=True):\n",
    "        super(UnetUp3_CT, self).__init__()\n",
    "        self.conv = UnetConv3(in_size + out_size, out_size, is_batchnorm, kernel_size=(3,3,3), padding_size=(1,1,1))\n",
    "        self.up = nn.Upsample(scale_factor=(2, 2, 2), mode='trilinear')\n",
    "\n",
    "        # initialise the blocks\n",
    "        for m in self.children():\n",
    "            if m.__class__.__name__.find('UnetConv3') != -1: continue\n",
    "            init_weights(m, init_type='kaiming')\n",
    "\n",
    "    def forward(self, inputs1, inputs2):\n",
    "        outputs2 = self.up(inputs2)\n",
    "        offset = outputs2.size()[2] - inputs1.size()[2]\n",
    "        padding = 2 * [offset // 2, offset // 2, 0]\n",
    "        outputs1 = F.pad(inputs1, padding)\n",
    "        return self.conv(torch.cat([outputs1, outputs2], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='attentionLayer'></a>\n",
    "### Grid Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _GridAttentionBlockND(nn.Module):\n",
    "    def __init__(self, in_channels, gating_channels, inter_channels=None, dimension=3, mode='concatenation',\n",
    "                 sub_sample_factor=(2,2,2)):\n",
    "        super(_GridAttentionBlockND, self).__init__()\n",
    "\n",
    "        assert dimension in [2, 3]\n",
    "        assert mode in ['concatenation', 'concatenation_debug', 'concatenation_residual']\n",
    "\n",
    "        # Downsampling rate for the input featuremap\n",
    "        if isinstance(sub_sample_factor, tuple): self.sub_sample_factor = sub_sample_factor\n",
    "        elif isinstance(sub_sample_factor, list): self.sub_sample_factor = tuple(sub_sample_factor)\n",
    "        else: self.sub_sample_factor = tuple([sub_sample_factor]) * dimension\n",
    "\n",
    "        # Default parameter set\n",
    "        self.mode = mode\n",
    "        self.dimension = dimension\n",
    "        self.sub_sample_kernel_size = self.sub_sample_factor\n",
    "\n",
    "        # Number of channels (pixel dimensions)\n",
    "        self.in_channels = in_channels\n",
    "        self.gating_channels = gating_channels\n",
    "        self.inter_channels = inter_channels\n",
    "\n",
    "        if self.inter_channels is None:\n",
    "            self.inter_channels = in_channels // 2\n",
    "            if self.inter_channels == 0:\n",
    "                self.inter_channels = 1\n",
    "\n",
    "        if dimension == 3:\n",
    "            conv_nd = nn.Conv3d\n",
    "            bn = nn.BatchNorm3d\n",
    "            self.upsample_mode = 'trilinear'\n",
    "        elif dimension == 2:\n",
    "            conv_nd = nn.Conv2d\n",
    "            bn = nn.BatchNorm2d\n",
    "            self.upsample_mode = 'bilinear'\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "\n",
    "        # Output transform\n",
    "        self.W = nn.Sequential(\n",
    "            conv_nd(in_channels=self.in_channels, out_channels=self.in_channels, kernel_size=1, stride=1, padding=0),\n",
    "            bn(self.in_channels),\n",
    "        )\n",
    "\n",
    "        # Theta^T * x_ij + Phi^T * gating_signal + bias\n",
    "        self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                             kernel_size=self.sub_sample_kernel_size, stride=self.sub_sample_factor, padding=0, bias=False)\n",
    "        self.phi = conv_nd(in_channels=self.gating_channels, out_channels=self.inter_channels,\n",
    "                           kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        self.psi = conv_nd(in_channels=self.inter_channels, out_channels=1, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "\n",
    "        # Initialise weights\n",
    "        for m in self.children():\n",
    "            init_weights(m, init_type='kaiming')\n",
    "\n",
    "        # Define the operation\n",
    "        if mode == 'concatenation':\n",
    "            self.operation_function = self._concatenation\n",
    "        elif mode == 'concatenation_debug':\n",
    "            self.operation_function = self._concatenation_debug\n",
    "        elif mode == 'concatenation_residual':\n",
    "            self.operation_function = self._concatenation_residual\n",
    "        else:\n",
    "            raise NotImplementedError('Unknown operation function.')\n",
    "\n",
    "\n",
    "    def forward(self, x, g):\n",
    "        '''\n",
    "        :param x: (b, c, t, h, w)\n",
    "        :param g: (b, g_d)\n",
    "        :return:\n",
    "        '''\n",
    "\n",
    "        output = self.operation_function(x, g)\n",
    "        return output\n",
    "\n",
    "    def _concatenation(self, x, g):\n",
    "        input_size = x.size()\n",
    "        batch_size = input_size[0]\n",
    "        assert batch_size == g.size(0)\n",
    "\n",
    "        # theta => (b, c, t, h, w) -> (b, i_c, t, h, w) -> (b, i_c, thw)\n",
    "        # phi   => (b, g_d) -> (b, i_c)\n",
    "        theta_x = self.theta(x)\n",
    "        theta_x_size = theta_x.size()\n",
    "\n",
    "        # g (b, c, t', h', w') -> phi_g (b, i_c, t', h', w')\n",
    "        #  Relu(theta_x + phi_g + bias) -> f = (b, i_c, thw) -> (b, i_c, t/s1, h/s2, w/s3)\n",
    "        phi_g = F.upsample(self.phi(g), size=theta_x_size[2:], mode=self.upsample_mode)\n",
    "        f = F.relu(theta_x + phi_g, inplace=True)\n",
    "\n",
    "        #  psi^T * f -> (b, psi_i_c, t/s1, h/s2, w/s3)\n",
    "        sigm_psi_f = F.sigmoid(self.psi(f))\n",
    "\n",
    "        # upsample the attentions and multiply\n",
    "        sigm_psi_f = F.upsample(sigm_psi_f, size=input_size[2:], mode=self.upsample_mode)\n",
    "        y = sigm_psi_f.expand_as(x) * x\n",
    "        W_y = self.W(y)\n",
    "\n",
    "        return W_y, sigm_psi_f\n",
    "\n",
    "    def _concatenation_debug(self, x, g):\n",
    "        input_size = x.size()\n",
    "        batch_size = input_size[0]\n",
    "        assert batch_size == g.size(0)\n",
    "\n",
    "        # theta => (b, c, t, h, w) -> (b, i_c, t, h, w) -> (b, i_c, thw)\n",
    "        # phi   => (b, g_d) -> (b, i_c)\n",
    "        theta_x = self.theta(x)\n",
    "        theta_x_size = theta_x.size()\n",
    "\n",
    "        # g (b, c, t', h', w') -> phi_g (b, i_c, t', h', w')\n",
    "        #  Relu(theta_x + phi_g + bias) -> f = (b, i_c, thw) -> (b, i_c, t/s1, h/s2, w/s3)\n",
    "        phi_g = F.upsample(self.phi(g), size=theta_x_size[2:], mode=self.upsample_mode)\n",
    "        f = F.softplus(theta_x + phi_g)\n",
    "\n",
    "        #  psi^T * f -> (b, psi_i_c, t/s1, h/s2, w/s3)\n",
    "        sigm_psi_f = F.sigmoid(self.psi(f))\n",
    "\n",
    "        # upsample the attentions and multiply\n",
    "        sigm_psi_f = F.upsample(sigm_psi_f, size=input_size[2:], mode=self.upsample_mode)\n",
    "        y = sigm_psi_f.expand_as(x) * x\n",
    "        W_y = self.W(y)\n",
    "\n",
    "        return W_y, sigm_psi_f\n",
    "\n",
    "\n",
    "    def _concatenation_residual(self, x, g):\n",
    "        input_size = x.size()\n",
    "        batch_size = input_size[0]\n",
    "        assert batch_size == g.size(0)\n",
    "\n",
    "        # theta => (b, c, t, h, w) -> (b, i_c, t, h, w) -> (b, i_c, thw)\n",
    "        # phi   => (b, g_d) -> (b, i_c)\n",
    "        theta_x = self.theta(x)\n",
    "        theta_x_size = theta_x.size()\n",
    "\n",
    "        # g (b, c, t', h', w') -> phi_g (b, i_c, t', h', w')\n",
    "        #  Relu(theta_x + phi_g + bias) -> f = (b, i_c, thw) -> (b, i_c, t/s1, h/s2, w/s3)\n",
    "        phi_g = F.upsample(self.phi(g), size=theta_x_size[2:], mode=self.upsample_mode)\n",
    "        f = F.relu(theta_x + phi_g, inplace=True)\n",
    "\n",
    "        #  psi^T * f -> (b, psi_i_c, t/s1, h/s2, w/s3)\n",
    "        f = self.psi(f).view(batch_size, 1, -1)\n",
    "        sigm_psi_f = F.softmax(f, dim=2).view(batch_size, 1, *theta_x.size()[2:])\n",
    "\n",
    "        # upsample the attentions and multiply\n",
    "        sigm_psi_f = F.upsample(sigm_psi_f, size=input_size[2:], mode=self.upsample_mode)\n",
    "        y = sigm_psi_f.expand_as(x) * x\n",
    "        W_y = self.W(y)\n",
    "\n",
    "        return W_y, sigm_psi_f\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
